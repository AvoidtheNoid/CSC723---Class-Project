{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was 'C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tf_encrypted/operations/secure_random/secure_random_module_tf_1.15.2.so'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tf_encrypted\\session.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Part X - Secure Training and Evaluation on MNIST\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import time\n",
    "import syft as sy  # import the Pysyft library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "# We don't use the whole dataset for efficiency purpose, but feel free to increase these numbers\n",
    "n_train_items = 640\n",
    "n_test_items = 640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 64\n",
    "        self.epochs = epochs\n",
    "        self.lr = 0.02\n",
    "        self.seed = 1\n",
    "        self.log_interval = 1 # Log info at each batch\n",
    "        self.precision_fractional = 3\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Arguments()\n",
    "\n",
    "_ = torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Encrypted Training demo on MNIST\n",
    "\n",
    "hook = sy.TorchHook(torch)  # hook PyTorch to add extra functionalities like Federated and Encrypted Learning\n",
    "\n",
    "# simulation functions\n",
    "def connect_to_workers(n_workers):\n",
    "    return [\n",
    "        sy.VirtualWorker(hook, id=f\"worker{i+1}\")\n",
    "        for i in range(n_workers)\n",
    "    ]\n",
    "def connect_to_crypto_provider():\n",
    "    return sy.VirtualWorker(hook, id=\"crypto_provider\")\n",
    "\n",
    "workers = connect_to_workers(n_workers=2)\n",
    "crypto_provider = connect_to_crypto_provider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_private_data_loaders(precision_fractional, workers, crypto_provider):\n",
    "    \n",
    "    def one_hot_of(index_tensor):\n",
    "        \"\"\"\n",
    "        Transform to one hot tensor\n",
    "        \n",
    "        Example:\n",
    "            [0, 3, 9]\n",
    "            =>\n",
    "            [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "             [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "             [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]\n",
    "            \n",
    "        \"\"\"\n",
    "        onehot_tensor = torch.zeros(*index_tensor.shape, 10) # 10 classes for MNIST\n",
    "        onehot_tensor = onehot_tensor.scatter(1, index_tensor.view(-1, 1), 1)\n",
    "        return onehot_tensor\n",
    "        \n",
    "    def secret_share(tensor):\n",
    "        \"\"\"\n",
    "        Transform to fixed precision and secret share a tensor\n",
    "        \"\"\"\n",
    "        return (\n",
    "            tensor\n",
    "            .fix_precision(precision_fractional=precision_fractional)\n",
    "            .share(*workers, crypto_provider=crypto_provider, requires_grad=True)\n",
    "        )\n",
    "    \n",
    "    transformation = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True, transform=transformation),\n",
    "        batch_size=args.batch_size\n",
    "    )\n",
    "    \n",
    "    private_train_loader = [\n",
    "        (secret_share(data), secret_share(one_hot_of(target)))\n",
    "        for i, (data, target) in enumerate(train_loader)\n",
    "        if i < n_train_items / args.batch_size\n",
    "    ]\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, download=True, transform=transformation),\n",
    "        batch_size=args.test_batch_size\n",
    "    )\n",
    "    \n",
    "    private_test_loader = [\n",
    "        (secret_share(data), secret_share(target.float()))\n",
    "        for i, (data, target) in enumerate(test_loader)\n",
    "        if i < n_test_items / args.test_batch_size\n",
    "    ]\n",
    "    \n",
    "    return private_train_loader, private_test_loader\n",
    "    \n",
    "    \n",
    "private_train_loader, private_test_loader = get_private_data_loaders(\n",
    "    precision_fractional=args.precision_fractional,\n",
    "    workers=workers,\n",
    "    crypto_provider=crypto_provider\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, private_train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(private_train_loader): # <-- now it is a private dataset\n",
    "        start_time = time.time()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        # loss = F.nll_loss(output, target)  <-- not possible here\n",
    "        batch_size = output.shape[0]\n",
    "        loss = ((output - target)**2).sum().refresh()/batch_size\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get().float_precision()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTime: {:.3f}s'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(private_train_loader) * args.batch_size,\n",
    "                100. * batch_idx / len(private_train_loader), loss.item(), time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, private_test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in private_test_loader:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target.view_as(pred)).sum()\n",
    "\n",
    "    correct = correct.get().float_precision()\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct.item(), len(private_test_loader)* args.test_batch_size,\n",
    "        100. * correct.item() / (len(private_test_loader) * args.test_batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/640 (0%)]\tLoss: 1.128000\tTime: 6.860s\n",
      "Train Epoch: 1 [64/640 (10%)]\tLoss: 1.012000\tTime: 6.829s\n",
      "Train Epoch: 1 [128/640 (20%)]\tLoss: 0.989000\tTime: 7.846s\n",
      "Train Epoch: 1 [192/640 (30%)]\tLoss: 0.902000\tTime: 7.737s\n",
      "Train Epoch: 1 [256/640 (40%)]\tLoss: 0.888000\tTime: 7.299s\n",
      "Train Epoch: 1 [320/640 (50%)]\tLoss: 0.876000\tTime: 7.487s\n",
      "Train Epoch: 1 [384/640 (60%)]\tLoss: 0.854000\tTime: 7.983s\n",
      "Train Epoch: 1 [448/640 (70%)]\tLoss: 0.853000\tTime: 7.502s\n",
      "Train Epoch: 1 [512/640 (80%)]\tLoss: 0.829000\tTime: 7.939s\n",
      "Train Epoch: 1 [576/640 (90%)]\tLoss: 0.841000\tTime: 7.310s\n",
      "\n",
      "Test set: Accuracy: 227.0/640 (35%)\n",
      "\n",
      "Train Epoch: 2 [0/640 (0%)]\tLoss: 0.781000\tTime: 7.280s\n",
      "Train Epoch: 2 [64/640 (10%)]\tLoss: 0.733000\tTime: 7.340s\n",
      "Train Epoch: 2 [128/640 (20%)]\tLoss: 0.791000\tTime: 7.443s\n",
      "Train Epoch: 2 [192/640 (30%)]\tLoss: 0.717000\tTime: 7.547s\n",
      "Train Epoch: 2 [256/640 (40%)]\tLoss: 0.707000\tTime: 8.083s\n",
      "Train Epoch: 2 [320/640 (50%)]\tLoss: 0.706000\tTime: 9.379s\n",
      "Train Epoch: 2 [384/640 (60%)]\tLoss: 0.709000\tTime: 7.845s\n",
      "Train Epoch: 2 [448/640 (70%)]\tLoss: 0.721000\tTime: 7.631s\n",
      "Train Epoch: 2 [512/640 (80%)]\tLoss: 0.710000\tTime: 8.157s\n",
      "Train Epoch: 2 [576/640 (90%)]\tLoss: 0.743000\tTime: 7.971s\n",
      "\n",
      "Test set: Accuracy: 360.0/640 (56%)\n",
      "\n",
      "Train Epoch: 3 [0/640 (0%)]\tLoss: 0.667000\tTime: 8.913s\n",
      "Train Epoch: 3 [64/640 (10%)]\tLoss: 0.596000\tTime: 7.963s\n",
      "Train Epoch: 3 [128/640 (20%)]\tLoss: 0.692000\tTime: 7.703s\n",
      "Train Epoch: 3 [192/640 (30%)]\tLoss: 0.600000\tTime: 7.855s\n",
      "Train Epoch: 3 [256/640 (40%)]\tLoss: 0.589000\tTime: 7.803s\n",
      "Train Epoch: 3 [320/640 (50%)]\tLoss: 0.590000\tTime: 7.491s\n",
      "Train Epoch: 3 [384/640 (60%)]\tLoss: 0.606000\tTime: 8.026s\n",
      "Train Epoch: 3 [448/640 (70%)]\tLoss: 0.628000\tTime: 7.933s\n",
      "Train Epoch: 3 [512/640 (80%)]\tLoss: 0.619000\tTime: 8.351s\n",
      "Train Epoch: 3 [576/640 (90%)]\tLoss: 0.668000\tTime: 7.757s\n",
      "\n",
      "Test set: Accuracy: 401.0/640 (63%)\n",
      "\n",
      "Train Epoch: 4 [0/640 (0%)]\tLoss: 0.584000\tTime: 8.445s\n",
      "Train Epoch: 4 [64/640 (10%)]\tLoss: 0.499000\tTime: 8.525s\n",
      "Train Epoch: 4 [128/640 (20%)]\tLoss: 0.618000\tTime: 8.290s\n",
      "Train Epoch: 4 [192/640 (30%)]\tLoss: 0.518000\tTime: 8.990s\n",
      "Train Epoch: 4 [256/640 (40%)]\tLoss: 0.512000\tTime: 7.888s\n",
      "Train Epoch: 4 [320/640 (50%)]\tLoss: 0.511000\tTime: 8.504s\n",
      "Train Epoch: 4 [384/640 (60%)]\tLoss: 0.535000\tTime: 8.408s\n",
      "Train Epoch: 4 [448/640 (70%)]\tLoss: 0.562000\tTime: 8.494s\n",
      "Train Epoch: 4 [512/640 (80%)]\tLoss: 0.552000\tTime: 8.405s\n",
      "Train Epoch: 4 [576/640 (90%)]\tLoss: 0.611000\tTime: 8.219s\n",
      "\n",
      "Test set: Accuracy: 424.0/640 (66%)\n",
      "\n",
      "Train Epoch: 5 [0/640 (0%)]\tLoss: 0.525000\tTime: 8.029s\n",
      "Train Epoch: 5 [64/640 (10%)]\tLoss: 0.435000\tTime: 8.600s\n",
      "Train Epoch: 5 [128/640 (20%)]\tLoss: 0.559000\tTime: 8.526s\n",
      "Train Epoch: 5 [192/640 (30%)]\tLoss: 0.459000\tTime: 8.533s\n",
      "Train Epoch: 5 [256/640 (40%)]\tLoss: 0.454000\tTime: 8.606s\n",
      "Train Epoch: 5 [320/640 (50%)]\tLoss: 0.451000\tTime: 10.043s\n",
      "Train Epoch: 5 [384/640 (60%)]\tLoss: 0.480000\tTime: 8.940s\n",
      "Train Epoch: 5 [448/640 (70%)]\tLoss: 0.510000\tTime: 8.235s\n",
      "Train Epoch: 5 [512/640 (80%)]\tLoss: 0.501000\tTime: 8.418s\n",
      "Train Epoch: 5 [576/640 (90%)]\tLoss: 0.567000\tTime: 8.658s\n",
      "\n",
      "Test set: Accuracy: 449.0/640 (70%)\n",
      "\n",
      "Train Epoch: 6 [0/640 (0%)]\tLoss: 0.476000\tTime: 8.777s\n",
      "Train Epoch: 6 [64/640 (10%)]\tLoss: 0.387000\tTime: 9.421s\n",
      "Train Epoch: 6 [128/640 (20%)]\tLoss: 0.516000\tTime: 9.780s\n",
      "Train Epoch: 6 [192/640 (30%)]\tLoss: 0.410000\tTime: 9.495s\n",
      "Train Epoch: 6 [256/640 (40%)]\tLoss: 0.412000\tTime: 10.051s\n",
      "Train Epoch: 6 [320/640 (50%)]\tLoss: 0.406000\tTime: 9.692s\n",
      "Train Epoch: 6 [384/640 (60%)]\tLoss: 0.438000\tTime: 11.094s\n",
      "Train Epoch: 6 [448/640 (70%)]\tLoss: 0.471000\tTime: 10.012s\n",
      "Train Epoch: 6 [512/640 (80%)]\tLoss: 0.462000\tTime: 9.816s\n",
      "Train Epoch: 6 [576/640 (90%)]\tLoss: 0.529000\tTime: 9.960s\n",
      "\n",
      "Test set: Accuracy: 464.0/640 (72%)\n",
      "\n",
      "Train Epoch: 7 [0/640 (0%)]\tLoss: 0.434000\tTime: 11.088s\n",
      "Train Epoch: 7 [64/640 (10%)]\tLoss: 0.352000\tTime: 11.074s\n",
      "Train Epoch: 7 [128/640 (20%)]\tLoss: 0.476000\tTime: 9.977s\n",
      "Train Epoch: 7 [192/640 (30%)]\tLoss: 0.378000\tTime: 12.809s\n",
      "Train Epoch: 7 [256/640 (40%)]\tLoss: 0.375000\tTime: 16.330s\n",
      "Train Epoch: 7 [320/640 (50%)]\tLoss: 0.368000\tTime: 11.432s\n",
      "Train Epoch: 7 [384/640 (60%)]\tLoss: 0.403000\tTime: 12.173s\n",
      "Train Epoch: 7 [448/640 (70%)]\tLoss: 0.440000\tTime: 18.950s\n",
      "Train Epoch: 7 [512/640 (80%)]\tLoss: 0.428000\tTime: 18.816s\n",
      "Train Epoch: 7 [576/640 (90%)]\tLoss: 0.499000\tTime: 16.041s\n",
      "\n",
      "Test set: Accuracy: 469.0/640 (73%)\n",
      "\n",
      "Train Epoch: 8 [0/640 (0%)]\tLoss: 0.407000\tTime: 17.960s\n",
      "Train Epoch: 8 [64/640 (10%)]\tLoss: 0.323000\tTime: 15.312s\n",
      "Train Epoch: 8 [128/640 (20%)]\tLoss: 0.447000\tTime: 16.963s\n",
      "Train Epoch: 8 [192/640 (30%)]\tLoss: 0.349000\tTime: 16.017s\n",
      "Train Epoch: 8 [256/640 (40%)]\tLoss: 0.348000\tTime: 15.624s\n",
      "Train Epoch: 8 [320/640 (50%)]\tLoss: 0.342000\tTime: 13.431s\n",
      "Train Epoch: 8 [384/640 (60%)]\tLoss: 0.375000\tTime: 11.459s\n",
      "Train Epoch: 8 [448/640 (70%)]\tLoss: 0.411000\tTime: 10.441s\n",
      "Train Epoch: 8 [512/640 (80%)]\tLoss: 0.403000\tTime: 11.372s\n",
      "Train Epoch: 8 [576/640 (90%)]\tLoss: 0.475000\tTime: 12.234s\n",
      "\n",
      "Test set: Accuracy: 474.0/640 (74%)\n",
      "\n",
      "Train Epoch: 9 [0/640 (0%)]\tLoss: 0.384000\tTime: 12.692s\n",
      "Train Epoch: 9 [64/640 (10%)]\tLoss: 0.301000\tTime: 12.582s\n",
      "Train Epoch: 9 [128/640 (20%)]\tLoss: 0.421000\tTime: 13.457s\n",
      "Train Epoch: 9 [192/640 (30%)]\tLoss: 0.327000\tTime: 12.831s\n",
      "Train Epoch: 9 [256/640 (40%)]\tLoss: 0.325000\tTime: 14.218s\n",
      "Train Epoch: 9 [320/640 (50%)]\tLoss: 0.318000\tTime: 12.200s\n",
      "Train Epoch: 9 [384/640 (60%)]\tLoss: 0.353000\tTime: 14.180s\n",
      "Train Epoch: 9 [448/640 (70%)]\tLoss: 0.391000\tTime: 14.024s\n",
      "Train Epoch: 9 [512/640 (80%)]\tLoss: 0.379000\tTime: 13.160s\n",
      "Train Epoch: 9 [576/640 (90%)]\tLoss: 0.455000\tTime: 12.714s\n",
      "\n",
      "Test set: Accuracy: 481.0/640 (75%)\n",
      "\n",
      "Train Epoch: 10 [0/640 (0%)]\tLoss: 0.363000\tTime: 14.244s\n",
      "Train Epoch: 10 [64/640 (10%)]\tLoss: 0.281000\tTime: 20.560s\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model = model.fix_precision().share(*workers, crypto_provider=crypto_provider, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "optimizer = optimizer.fix_precision() \n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, private_train_loader, optimizer, epoch)\n",
    "    test(args, model, private_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
