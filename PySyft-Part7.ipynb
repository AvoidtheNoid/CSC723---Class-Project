{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was 'C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tf_encrypted/operations/secure_random/secure_random_module_tf_1.15.2.so'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tf_encrypted\\session.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Setting up Sandbox...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Part 7 - Federated Learning with FederatedDataset\n",
    "\n",
    "import torch as th\n",
    "import syft as sy\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "sy.create_sandbox(globals(), verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_data = grid.search(\"#boston\", \"#data\")\n",
    "boston_target = grid.search(\"#boston\", \"#target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = boston_data['alice'][0].shape[1]\n",
    "n_targets = 1\n",
    "\n",
    "model = th.nn.Linear(n_features, n_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bob', 'theo', 'jason', 'alice', 'andy', 'jon']\n"
     ]
    }
   ],
   "source": [
    "# Cast the result in BaseDatasets\n",
    "datasets = []\n",
    "for worker in boston_data.keys():\n",
    "    dataset = sy.BaseDataset(boston_data[worker][0], boston_target[worker][0])\n",
    "    datasets.append(dataset)\n",
    "\n",
    "# Build the FederatedDataset object\n",
    "dataset = sy.FederatedDataset(datasets)\n",
    "print(dataset.workers)\n",
    "optimizers = {}\n",
    "for worker in dataset.workers:\n",
    "    optimizers[worker] = th.optim.Adam(params=model.parameters(),lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = sy.FederatedDataLoader(dataset, batch_size=32, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/16 (0%)]\tBatch loss: 9823.008789\n",
      "Train Epoch: 1 [8/16 (50%)]\tBatch loss: 2321.514648\n",
      "Train Epoch: 1 [16/16 (100%)]\tBatch loss: 47.774078\n",
      "Total loss 63394.198066711426\n",
      "Train Epoch: 2 [0/16 (0%)]\tBatch loss: 120.189125\n",
      "Train Epoch: 2 [8/16 (50%)]\tBatch loss: 1038.533447\n",
      "Train Epoch: 2 [16/16 (100%)]\tBatch loss: 1471.783203\n",
      "Total loss 26244.85154724121\n",
      "Train Epoch: 3 [0/16 (0%)]\tBatch loss: 319.875244\n",
      "Train Epoch: 3 [8/16 (50%)]\tBatch loss: 371.299316\n",
      "Train Epoch: 3 [16/16 (100%)]\tBatch loss: 61.601097\n",
      "Total loss 7772.840141296387\n",
      "Train Epoch: 4 [0/16 (0%)]\tBatch loss: 205.691330\n",
      "Train Epoch: 4 [8/16 (50%)]\tBatch loss: 200.993286\n",
      "Train Epoch: 4 [16/16 (100%)]\tBatch loss: 804.882507\n",
      "Total loss 5748.429527282715\n",
      "Train Epoch: 5 [0/16 (0%)]\tBatch loss: 372.154938\n",
      "Train Epoch: 5 [8/16 (50%)]\tBatch loss: 202.719574\n",
      "Train Epoch: 5 [16/16 (100%)]\tBatch loss: 164.997635\n",
      "Total loss 4867.413619995117\n",
      "Train Epoch: 6 [0/16 (0%)]\tBatch loss: 82.889633\n",
      "Train Epoch: 6 [8/16 (50%)]\tBatch loss: 94.942398\n",
      "Train Epoch: 6 [16/16 (100%)]\tBatch loss: 74.237244\n",
      "Total loss 3238.661632537842\n",
      "Train Epoch: 7 [0/16 (0%)]\tBatch loss: 148.438156\n",
      "Train Epoch: 7 [8/16 (50%)]\tBatch loss: 158.867096\n",
      "Train Epoch: 7 [16/16 (100%)]\tBatch loss: 40.275539\n",
      "Total loss 3371.63334274292\n",
      "Train Epoch: 8 [0/16 (0%)]\tBatch loss: 91.891579\n",
      "Train Epoch: 8 [8/16 (50%)]\tBatch loss: 82.390038\n",
      "Train Epoch: 8 [16/16 (100%)]\tBatch loss: 136.360413\n",
      "Total loss 1943.2082958221436\n",
      "Train Epoch: 9 [0/16 (0%)]\tBatch loss: 69.072937\n",
      "Train Epoch: 9 [8/16 (50%)]\tBatch loss: 89.977913\n",
      "Train Epoch: 9 [16/16 (100%)]\tBatch loss: 146.555954\n",
      "Total loss 2087.335329055786\n",
      "Train Epoch: 10 [0/16 (0%)]\tBatch loss: 63.550720\n",
      "Train Epoch: 10 [8/16 (50%)]\tBatch loss: 80.297684\n",
      "Train Epoch: 10 [16/16 (100%)]\tBatch loss: 46.475769\n",
      "Total loss 1785.793966293335\n",
      "Train Epoch: 11 [0/16 (0%)]\tBatch loss: 85.108498\n",
      "Train Epoch: 11 [8/16 (50%)]\tBatch loss: 95.993813\n",
      "Train Epoch: 11 [16/16 (100%)]\tBatch loss: 35.505676\n",
      "Total loss 1861.7518825531006\n",
      "Train Epoch: 12 [0/16 (0%)]\tBatch loss: 87.906609\n",
      "Train Epoch: 12 [8/16 (50%)]\tBatch loss: 87.394241\n",
      "Train Epoch: 12 [16/16 (100%)]\tBatch loss: 50.023861\n",
      "Total loss 1673.6134433746338\n",
      "Train Epoch: 13 [0/16 (0%)]\tBatch loss: 64.516212\n",
      "Train Epoch: 13 [8/16 (50%)]\tBatch loss: 77.879318\n",
      "Train Epoch: 13 [16/16 (100%)]\tBatch loss: 60.085155\n",
      "Total loss 1600.6894130706787\n",
      "Train Epoch: 14 [0/16 (0%)]\tBatch loss: 59.034058\n",
      "Train Epoch: 14 [8/16 (50%)]\tBatch loss: 76.857178\n",
      "Train Epoch: 14 [16/16 (100%)]\tBatch loss: 38.726009\n",
      "Total loss 1597.2300243377686\n",
      "Train Epoch: 15 [0/16 (0%)]\tBatch loss: 62.767078\n",
      "Train Epoch: 15 [8/16 (50%)]\tBatch loss: 79.762360\n",
      "Train Epoch: 15 [16/16 (100%)]\tBatch loss: 30.562454\n",
      "Total loss 1606.8273658752441\n",
      "Train Epoch: 16 [0/16 (0%)]\tBatch loss: 61.101379\n",
      "Train Epoch: 16 [8/16 (50%)]\tBatch loss: 77.877304\n",
      "Train Epoch: 16 [16/16 (100%)]\tBatch loss: 32.086861\n",
      "Total loss 1565.678171157837\n",
      "Train Epoch: 17 [0/16 (0%)]\tBatch loss: 53.659660\n",
      "Train Epoch: 17 [8/16 (50%)]\tBatch loss: 73.439041\n",
      "Train Epoch: 17 [16/16 (100%)]\tBatch loss: 33.706131\n",
      "Total loss 1544.9303016662598\n",
      "Train Epoch: 18 [0/16 (0%)]\tBatch loss: 50.398594\n",
      "Train Epoch: 18 [8/16 (50%)]\tBatch loss: 72.460663\n",
      "Train Epoch: 18 [16/16 (100%)]\tBatch loss: 29.424643\n",
      "Total loss 1554.561752319336\n",
      "Train Epoch: 19 [0/16 (0%)]\tBatch loss: 49.037621\n",
      "Train Epoch: 19 [8/16 (50%)]\tBatch loss: 73.645218\n",
      "Train Epoch: 19 [16/16 (100%)]\tBatch loss: 27.075871\n",
      "Total loss 1545.1566333770752\n",
      "Train Epoch: 20 [0/16 (0%)]\tBatch loss: 47.375736\n",
      "Train Epoch: 20 [8/16 (50%)]\tBatch loss: 73.035843\n",
      "Train Epoch: 20 [16/16 (100%)]\tBatch loss: 28.103292\n",
      "Total loss 1511.085443496704\n",
      "Train Epoch: 21 [0/16 (0%)]\tBatch loss: 45.768005\n",
      "Train Epoch: 21 [8/16 (50%)]\tBatch loss: 71.428276\n",
      "Train Epoch: 21 [16/16 (100%)]\tBatch loss: 28.765936\n",
      "Total loss 1480.7734470367432\n",
      "Train Epoch: 22 [0/16 (0%)]\tBatch loss: 44.490479\n",
      "Train Epoch: 22 [8/16 (50%)]\tBatch loss: 71.344223\n",
      "Train Epoch: 22 [16/16 (100%)]\tBatch loss: 27.441236\n",
      "Total loss 1453.76025390625\n",
      "Train Epoch: 23 [0/16 (0%)]\tBatch loss: 43.133114\n",
      "Train Epoch: 23 [8/16 (50%)]\tBatch loss: 71.745750\n",
      "Train Epoch: 23 [16/16 (100%)]\tBatch loss: 26.683393\n",
      "Total loss 1418.5459690093994\n",
      "Train Epoch: 24 [0/16 (0%)]\tBatch loss: 41.838673\n",
      "Train Epoch: 24 [8/16 (50%)]\tBatch loss: 70.967087\n",
      "Train Epoch: 24 [16/16 (100%)]\tBatch loss: 26.732546\n",
      "Total loss 1380.0262470245361\n",
      "Train Epoch: 25 [0/16 (0%)]\tBatch loss: 40.621044\n",
      "Train Epoch: 25 [8/16 (50%)]\tBatch loss: 69.842575\n",
      "Train Epoch: 25 [16/16 (100%)]\tBatch loss: 26.076651\n",
      "Total loss 1347.666187286377\n",
      "Train Epoch: 26 [0/16 (0%)]\tBatch loss: 39.426884\n",
      "Train Epoch: 26 [8/16 (50%)]\tBatch loss: 69.193726\n",
      "Train Epoch: 26 [16/16 (100%)]\tBatch loss: 24.849773\n",
      "Total loss 1320.5889892578125\n",
      "Train Epoch: 27 [0/16 (0%)]\tBatch loss: 38.307335\n",
      "Train Epoch: 27 [8/16 (50%)]\tBatch loss: 68.454247\n",
      "Train Epoch: 27 [16/16 (100%)]\tBatch loss: 23.929955\n",
      "Total loss 1295.6964225769043\n",
      "Train Epoch: 28 [0/16 (0%)]\tBatch loss: 37.293339\n",
      "Train Epoch: 28 [8/16 (50%)]\tBatch loss: 67.388687\n",
      "Train Epoch: 28 [16/16 (100%)]\tBatch loss: 23.175112\n",
      "Total loss 1274.159267425537\n",
      "Train Epoch: 29 [0/16 (0%)]\tBatch loss: 36.369362\n",
      "Train Epoch: 29 [8/16 (50%)]\tBatch loss: 66.486221\n",
      "Train Epoch: 29 [16/16 (100%)]\tBatch loss: 22.287809\n",
      "Total loss 1256.808219909668\n",
      "Train Epoch: 30 [0/16 (0%)]\tBatch loss: 35.520760\n",
      "Train Epoch: 30 [8/16 (50%)]\tBatch loss: 65.861748\n",
      "Train Epoch: 30 [16/16 (100%)]\tBatch loss: 21.472305\n",
      "Total loss 1241.4234447479248\n",
      "Train Epoch: 31 [0/16 (0%)]\tBatch loss: 34.753090\n",
      "Train Epoch: 31 [8/16 (50%)]\tBatch loss: 65.280090\n",
      "Train Epoch: 31 [16/16 (100%)]\tBatch loss: 20.856979\n",
      "Total loss 1226.4145526885986\n",
      "Train Epoch: 32 [0/16 (0%)]\tBatch loss: 34.067535\n",
      "Train Epoch: 32 [8/16 (50%)]\tBatch loss: 64.745468\n",
      "Train Epoch: 32 [16/16 (100%)]\tBatch loss: 20.311167\n",
      "Total loss 1211.8044147491455\n",
      "Train Epoch: 33 [0/16 (0%)]\tBatch loss: 33.440170\n",
      "Train Epoch: 33 [8/16 (50%)]\tBatch loss: 64.349426\n",
      "Train Epoch: 33 [16/16 (100%)]\tBatch loss: 19.781694\n",
      "Total loss 1197.2495670318604\n",
      "Train Epoch: 34 [0/16 (0%)]\tBatch loss: 32.852398\n",
      "Train Epoch: 34 [8/16 (50%)]\tBatch loss: 64.001251\n",
      "Train Epoch: 34 [16/16 (100%)]\tBatch loss: 19.314356\n",
      "Total loss 1182.206877708435\n",
      "Train Epoch: 35 [0/16 (0%)]\tBatch loss: 32.300217\n",
      "Train Epoch: 35 [8/16 (50%)]\tBatch loss: 63.608124\n",
      "Train Epoch: 35 [16/16 (100%)]\tBatch loss: 18.888096\n",
      "Total loss 1166.9275703430176\n",
      "Train Epoch: 36 [0/16 (0%)]\tBatch loss: 31.772251\n",
      "Train Epoch: 36 [8/16 (50%)]\tBatch loss: 63.186684\n",
      "Train Epoch: 36 [16/16 (100%)]\tBatch loss: 18.459770\n",
      "Total loss 1151.9179344177246\n",
      "Train Epoch: 37 [0/16 (0%)]\tBatch loss: 31.259571\n",
      "Train Epoch: 37 [8/16 (50%)]\tBatch loss: 62.741352\n",
      "Train Epoch: 37 [16/16 (100%)]\tBatch loss: 18.038731\n",
      "Total loss 1137.3917026519775\n",
      "Train Epoch: 38 [0/16 (0%)]\tBatch loss: 30.766247\n",
      "Train Epoch: 38 [8/16 (50%)]\tBatch loss: 62.249249\n",
      "Train Epoch: 38 [16/16 (100%)]\tBatch loss: 17.639664\n",
      "Total loss 1123.5237350463867\n",
      "Train Epoch: 39 [0/16 (0%)]\tBatch loss: 30.296009\n",
      "Train Epoch: 39 [8/16 (50%)]\tBatch loss: 61.726906\n",
      "Train Epoch: 39 [16/16 (100%)]\tBatch loss: 17.257463\n",
      "Total loss 1110.5014295578003\n",
      "Train Epoch: 40 [0/16 (0%)]\tBatch loss: 29.848083\n",
      "Train Epoch: 40 [8/16 (50%)]\tBatch loss: 61.205139\n",
      "Train Epoch: 40 [16/16 (100%)]\tBatch loss: 16.894695\n",
      "Total loss 1098.3150129318237\n",
      "Train Epoch: 41 [0/16 (0%)]\tBatch loss: 29.423004\n",
      "Train Epoch: 41 [8/16 (50%)]\tBatch loss: 60.692482\n",
      "Train Epoch: 41 [16/16 (100%)]\tBatch loss: 16.559488\n",
      "Total loss 1086.8194122314453\n",
      "Train Epoch: 42 [0/16 (0%)]\tBatch loss: 29.021023\n",
      "Train Epoch: 42 [8/16 (50%)]\tBatch loss: 60.193390\n",
      "Train Epoch: 42 [16/16 (100%)]\tBatch loss: 16.250212\n",
      "Total loss 1075.8853969573975\n",
      "Train Epoch: 43 [0/16 (0%)]\tBatch loss: 28.639660\n",
      "Train Epoch: 43 [8/16 (50%)]\tBatch loss: 59.715431\n",
      "Train Epoch: 43 [16/16 (100%)]\tBatch loss: 15.961756\n",
      "Total loss 1065.3982019424438\n",
      "Train Epoch: 44 [0/16 (0%)]\tBatch loss: 28.276386\n",
      "Train Epoch: 44 [8/16 (50%)]\tBatch loss: 59.256508\n",
      "Train Epoch: 44 [16/16 (100%)]\tBatch loss: 15.691597\n",
      "Total loss 1055.2566108703613\n",
      "Train Epoch: 45 [0/16 (0%)]\tBatch loss: 27.929867\n",
      "Train Epoch: 45 [8/16 (50%)]\tBatch loss: 58.809593\n",
      "Train Epoch: 45 [16/16 (100%)]\tBatch loss: 15.436330\n",
      "Total loss 1045.4186840057373\n",
      "Train Epoch: 46 [0/16 (0%)]\tBatch loss: 27.598930\n",
      "Train Epoch: 46 [8/16 (50%)]\tBatch loss: 58.371410\n",
      "Train Epoch: 46 [16/16 (100%)]\tBatch loss: 15.192031\n",
      "Total loss 1035.8903360366821\n",
      "Train Epoch: 47 [0/16 (0%)]\tBatch loss: 27.282751\n",
      "Train Epoch: 47 [8/16 (50%)]\tBatch loss: 57.940319\n",
      "Train Epoch: 47 [16/16 (100%)]\tBatch loss: 14.957192\n",
      "Total loss 1026.687952041626\n",
      "Train Epoch: 48 [0/16 (0%)]\tBatch loss: 26.981091\n",
      "Train Epoch: 48 [8/16 (50%)]\tBatch loss: 57.515205\n",
      "Train Epoch: 48 [16/16 (100%)]\tBatch loss: 14.731592\n",
      "Total loss 1017.8253335952759\n",
      "Train Epoch: 49 [0/16 (0%)]\tBatch loss: 26.693722\n",
      "Train Epoch: 49 [8/16 (50%)]\tBatch loss: 57.097542\n",
      "Train Epoch: 49 [16/16 (100%)]\tBatch loss: 14.515139\n",
      "Total loss 1009.3079137802124\n",
      "Train Epoch: 50 [0/16 (0%)]\tBatch loss: 26.420046\n",
      "Train Epoch: 50 [8/16 (50%)]\tBatch loss: 56.689735\n",
      "Train Epoch: 50 [16/16 (100%)]\tBatch loss: 14.308085\n",
      "Total loss 1001.1208877563477\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    loss_accum = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        model.send(data.location)\n",
    "        \n",
    "        optimizer = optimizers[data.location.id]\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(data)\n",
    "        loss = ((pred.view(-1) - target)**2).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.get()\n",
    "        loss = loss.get()\n",
    "        \n",
    "        loss_accum += float(loss)\n",
    "        \n",
    "        if batch_idx % 8 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tBatch loss: {:.6f}'.format(\n",
    "                epoch, batch_idx, len(train_loader),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))            \n",
    "            \n",
    "    print('Total loss', loss_accum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
